<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[How to build a self-driving car]]></title>
    <url>%2F2020%2F02%2F22%2FHow-to-build-a-self-driving-car%2F</url>
    <content type="text"><![CDATA[This is the blog to record this project. The code page is here. ðŸš© ç›®æ ‡ï¼šåœ¨ä¸€å¹´å†…åšä¸€ä¸ªåŸºäºŽè§†è§‰çš„åµŒå…¥å¼è‡ªåŠ¨é©¾é©¶å°è½¦ðŸš— ï¼ŒåŒ…æ‹¬å¤šå±‚æ¬¡çš„æ„ŸçŸ¥å’Œå†³ç­– ðŸ’» ä»£ç ï¼šhttps://github.com/UltronAI/self-driving-car è¿›åº¦ ç¡®å®šç¡¬ä»¶æ–¹æ¡ˆ ç¡¬ä»¶å¹³å° Nvidia Jetson Nano ? å­˜å‚¨å’ŒCPU ? æ„ŸçŸ¥ç®—æ³•å†³ç­–ç®—æ³•æµ‹è¯•åœºæ™¯ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>project</category>
        <category>in Chinese</category>
      </categories>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Repost] What Is Processing-in-Memory (PIM)?]]></title>
    <url>%2F2019%2F09%2F01%2FRepost-What-Is-Processing-in-Memory-PIM%2F</url>
    <content type="text"><![CDATA[Reposted from SIGDA Newsletter Seq 1, 2019. 1234Po-Chun Huang, Assistant Professor, po.chun.huang@mail.ntut.edu.twNational Taipei University of Technology, Taiwan Classical computer architecture used to adopt separate hardware components for computation (e.g., CPUs and GPGPUs) and those for data storage (e.g., DRAM-based main memory or disk-based secondary storage) [1]. As a result, the performance of modern data-intensive applications is often limited by the speeds of the memory/storage devices, as well as the bandwidth of the datapath between CPUs/GPUs and memory/storage, thereby creating the well-known von Neumann bottleneck [2]. To overcome this problem, the processing in memory (PIM) technology is proposed to incorporate the processing capability to random-access memory (RAM) in a single chip [2]. By moving some computation from processor to memory [3], the overheads to transmit data between processor and memory can be remarkably alleviated. As reported by prior literature [2], the PIM technology becomes more feasible as the main memory capacity increases, and are especially valuable for the emerging data-centric computing scenarios, such as data mining and machine learning applications. As early proposals of PIM introduce fully programmable computation units (e.g., general-purpose processors or field programmable gate arrays) to the memory, the design efforts could be high, and the changes to the hardware/software stack might be inevitable [7]. Thus, the recent development of PIM technology is driven by the emergence of modern nonvolatile memories (NVMs), such as phase-change memory (PCM), metal-oxide resistive RAM (ReRAM), and spin-transfer torque RAM (STT-RAM), which can directly perform logical and arithmetic operations in memory [1]. Among these choices, ReRAM can support efficient matrixâ€“vector multiply operations [1], and is widely used for neural computation [4], graphic algorithms [5], or performing bulk bitwise operations [6]. Nevertheless, the PIM technology is not preferred by all flavors of computations against CPUs/GPUs, and extra research efforts are needed to fully unleash its power. Observing the heterogeneous computation capabilities of CPUs/GPGPUs and those of PIM, it is a brilliant approach to wisely determine whether to execute specific instructions, referred to as PIM-enabled instructions (PEIs), by PIM on the main memory, as suggested by Ahn et al. [7]. With PEIs, programmers can assign the instructions that should be executed by PIM, therefore optimizing the system performance. Representative examples of PEIs include the increment of integers, getting the minimum element, addition of floating-point numbers, computation of Euclidean distance, and computation of the dot product of vectors [7]. The PIM technology is especially suitable for applications in the artificial intelligence area, as observed by [1], [4], [5]. This is because that certain NVMs, such as ReRAM, can inherently support neural computation with its crossbar physical architecture. By allocating a full function (FF) subarray in the ReRAM space [1], a remarkable improvement of energy saving is observed at slight area overheads. Because the FF subarray is established dynamically in the ReRAM, the dynamic morphing of the ReRAM space for storing data and that for keeping the FF subarray becomes possible, allowing further performance enhancements. In next-generation computing systems with explosive scales, the datapath between processing components and memory/storage components might become one of the major performance bottlenecks. In this case, PIM is promising to assist the establishing of active memory cubes (AMCs) [8], which provides not only data storage but also energy-efficient computation functionalities in a single component. A plural of AMCs can then be interconnected to construct a coherent and scalable main memory device for performance-demanding applications such as scientific computation [8]. Furthermore, the importance of the PIM technology is expected to be emphasized in the foreseeable future, due to the rapidly growing scale of computing systems. As new applications keep emerging on the horizon, existing computer architecture is pushed further to deliver better performance and energy efficiency for the whole system. Among the potential technologies, PIM provides amazingly high performance and energy efficiency, and is a promising candidate for the key technologies in data-centric computing scenarios. We believe that the PIM technology is still worth more research attentions to reveal its value to a wider spectrum of applications. [1] Chi, Ping, et al. â€œPrime: A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory.â€ ACM SIGARCH Computer Architecture News. Vol. 44. No. 3. IEEE Press, 2016. [2] Margaret Rouse, â€œWhat is processing in memory (PIM)?,â€ online available at: https://searchbusinessanalytics.techtarget.com/definition/processing-in-memory-PIM [3] Xu Yang, Yumin Hou, and Hu He, â€œA Processing-in-Memory Architecture Programming Paradigm for Wireless Internet-of-Things Applications,â€ MDPI Sensors, Vol. 19, No. 140, 2019. [4] Song, Linghao, et al. â€œPipelayer: A pipelined ReRAM-based accelerator for deep learning.â€ 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2017. [5] Long, Yun, Taesik Na, and Saibal Mukhopadhyay. â€œReRAM-based processing-in-memory architecture for recurrent neural network acceleration.â€ IEEE Transactions on Very Large Scale Integration (VLSI) Systems 26.12 (2018): 2781-2794. [6] Li, Shuangchen, et al. â€œPinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories.â€ Proceedings of the 53rd Annual Design Automation Conference. ACM, 2016. [7] J. Ahn, S. Yoo, O. Mutlu and K. Choi, â€œPIM-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture,â€ 2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA), Portland, OR, 2015, pp. 336â€“348. [8] R. Nair et al., â€œActive Memory Cube: A processing-in-memory architecture for exascale systems,â€ IBM Journal of Research and Development, vol. 59, no. 2/3, pp. 17:1-17:14, Marchâ€“May 2015. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>What Is</category>
      </categories>
      <tags>
        <tag>hardware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some Key Publications of NICS-EFC Lab]]></title>
    <url>%2F2019%2F03%2F19%2FSome-Key-Publications-of-NICS-EFC-Lab%2F</url>
    <content type="text"><![CDATA[[CNN Accelerator on FPGA, FGPA 2016] Going Deeper with Embedded FPGA Platform for Convolutional Neural Network. [LSTM Accelerator on FPGA, FGPA 2017 Best Paper] ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA. [FPGA in the Cloud, CF 2014] Enabling FPGAs in the Cloud. [MapReduce on FPGA, FPGA 2010] FPMR: MapReduce Framework on FPGA A Case Study of RankBoost Acceleration. [PIM with ReRAM, ISCA 2016] PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory. 1. Qiu, J., Wang, J., Yao, S., Guo, K., Li, B., Zhou, E., Yu, J. Tang, T., Xu, N., Song, S., Wang, Yu (2016): Going Deeper with Embedded FPGA Platform for Convolutional Neural Network. Accepted in: The 24th ACM/SIGDA International Symposium on Field Programmable Gate Arrays (FPGAâ€™16).The CNN-based approach has achieved great success in many applications, however, the computational-intensive and resource-consuming nature of CNN makes it difficult to run CNN on embedded systems. In this paper, we make an in-depth investigation of the memory footprint and bandwidth problem in order to accelerate state-of-the-art CNN models for Imagenet classification on the embedded FPGA platform and show that CONV layers are computational-centric and Fully-Connected layers are memory-centric. In addition, a dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. A data arrangement method for FC layers is proposed to further ensure a high utilization of the external memory bandwidth. Empirical experiments have shown that the proposed method is very efficient, so CNN can run at high speed on the embedded platform without significantly reducing the accuracy. The analysis in this work makes key inspiration for the subsequent FPGA-based CNN accelerators. 2. Han, S., Kang, J., Mao, H., Hu, Y., Li, X., Li, Y, Xie, D., Luo, H., Yao, S., Wang, Yu, Yang, H., Dally, W. J. (2017): ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA. Accepted in: The 25th ACM/SIGDA International Symposium on Field Programmable Gate Arrays (FPGAâ€™17).Increasingly large LSTM models are used to improve the accuracy of speech recognition tasks, which is both computation and memory intensive and leads to high power consumption. In this paper, we propose a method to compress the LSTM model by $20\times$ with high hardware utilization while without sacrificing the prediction accuracy. Then we propose a scheduler that encodes and partitions the compressed model to multiple PEs for parallelism and schedules the complicated LSTM data flow. And a hardware architecture, named ESE, is designed to deals with the irregularity caused by compression, so that it can work directly on the sparse LSTM model. Experiments show that LSTM implemented on Xilinx FPGAs with ESE is superior to LSTM on top-level CPUs and GPUs, and has higher energy efficiency. This work allows us to not only accelerate CNN on the FPGA, but also accelerate LSTM, giving our FPGA-based accelerators a wider range of applications and higher scalability potential. 3. Chen, F., Shan, Y., Zhang, Y., Wang, Yu, Franke, H., Chang, X., Wang, K. (2014): Enabling FPGAs in the Cloud. Accepted in: Proceedings of the 11th ACM Conference on Computing Frontiers (CFâ€™14).Cloud computing is becoming a major trend for delivering and accessing infrastructure on demand via the network. Many types of workloads in the cloud can be accelerated by FPGAs, as FPGAs have the ability to achieve high throughput and predictable latency while providing programmability, low power consumption and time-to-value. However, integrating FPGAs into the cloud is nontrivial due to some FPGA-related issues. In this paper, we analyze the impediments to bringing FPGAs as a shareable resource to the cloud. To overcome these impediments, we provide a framework and a prototype to provide an FPGA cloud solution within the scope of FPGA technology at the time. The prototype enables isolation between multiple processes in multiple VMs, precise quantitative acceleration resource allocation, and priority-based workload scheduling. Given the prototype, we also demonstrate how abstraction, sharing, compatibility and security can be achieved while using FPGAs in the cloud. This work provides a viable solution to use FPGAs for computational acceleration in the cloud. 4. Shan, Y., Wang, B., Yan, J., Wang, Yu, Xu, N., Yang, H. (2010): FPMR: MapReduce Framework on FPGA A Case Study of RankBoost Acceleration. Accepted in: The 18th ACM/SIGDA International Symposium on Field Programmable Gate Arrays (FPGAâ€™10).FPGA provides a highly parallel, low power, and flexible hardware platform for machine learning and data mining, while the difficulty of programming FPGA greatly limits its prevalence. MapReduce is a parallel programming framework that could easily utilize inherent parallelism in algorithms. In this paper, we propose a MapReduce framework on FPGA, called FPMR, which provides programming abstraction, hardware architecture and basic building blocks to developers. High Parallelism can be easily achieved on FPMR, while the programming efforts are alleviated. Using this framework, designers only need to map the applications onto the mapper modules and the reducer modules. Task scheduling, communication, and data synchronization are done by the framework automatically. In addition, we discuss the tradeoffs among resources, performance, and memory bandwidth, and show that the bandwidth of memory will be the limiting factor during the application acceleration based on FPMR. This work can help developers to build and test machine learning accelerators on FPGAs faster, which is beneficial to the development of the community. 5. Chi, P., Li, S., Xu, C., Zhang, T., Zhao, J., Liu, Y., Wang, Yu, Xie, Y. (2016): PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory. Accepted in: The 43rd ACM/IEEE Annual International Symposium on Computer Architecture (ISCAâ€™16).Processing-in-memory (PIM) is a promising solution to address the â€œmemory wallâ€ challenges for future computer systems. Instead of putting additional computation logic in or near memory, we propose a novel PIM architecture, called PRIME, to accelerate NN applications in ReRAM-based main memory, where ReRAM is an emerging metal-oxide resistive random access memory. In PRIME, a portion of ReRAM crossbar arrays can be configured as accelerators for NN applications or as normal memory for a larger memory space, which takes advantage of both the PIM architecture and the efficiency of ReRAM-based computation. Then we present our designs from circuit-level to system-level and take experiments to demonstrate the ability of the proposed architecture to save energy and accelerate various NN applications using MLP and CNN. This work uses the PIM architecture to reduce memory limitations while speeding up NN applications, providing a new type of accelerator structure. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>paper-reading</category>
        <category>survey</category>
      </categories>
      <tags>
        <tag>NICS</tag>
        <tag>FPGA</tag>
        <tag>NN-accelerator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Monocular Visual-Odometry SLAM System]]></title>
    <url>%2F2019%2F03%2F18%2FMonocular-Visual-Odometry-SLAM-System%2F</url>
    <content type="text"><![CDATA[[In Chinese]Notes about some feature-based and NN-based Visual-Inertial Odometry Systems: On-Manifold Preintegration for Real-Time Visual-Inertial Odometry. (IEEE Trans. on Robotics) VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator. (IEEE Trans. on Robotics) Online Temporal Calibration for Monocular Visual-Inertial Systems. (IROS 2018) VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem. (AAAI 2017) Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction. (IROS 2018) Selective Sensor Fusion for Neural Visual-Inertial Odometry. (CVPR 2019) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>paper-reading</category>
        <category>in Chinese</category>
        <category>survey</category>
      </categories>
      <tags>
        <tag>VIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cardiopulmonary resuscitation (CPR)]]></title>
    <url>%2F2019%2F03%2F16%2FCardiopulmonary-resuscitation-CPR%2F</url>
    <content type="text"><![CDATA[[In Chinese]In case of emergency, how to maintain the life of the patient without breathing and heartbeat and wait for rescue ? å¿ƒè‚ºå¤è‹ï¼ˆCPRï¼‰ ç¡®è®¤æ‚£è€…ä¸‰æ— ï¼ˆæ— æ„è¯†ï¼Œæ— å‘¼å¸ï¼Œæ— å¿ƒè·³ï¼‰ä¹‹åŽï¼š å¤§å£°å‘¼å”¤ï¼Œå¯»æ±‚å¸®åŠ©ï¼ˆâ€œå¿«æ¥äººï¼Œè¿™é‡Œæœ‰äººéœ€è¦æ€¥æ•‘â€ï¼‰ è¯´æ˜Žèº«ä»½ï¼Œå»ºç«‹åˆæ­¥ä¿¡ä»»ï¼ˆâ€œæˆ‘å­¦è¿‡æ€¥æ•‘çŸ¥è¯†ï¼Œæˆ‘æ˜¯æ•‘æŠ¤å‘˜â€ï¼‰ æ‰¾äººæ‹¨æ‰“æ€¥æ•‘ç”µè¯ï¼Œå¹¶è¯¢é—®ç»“æžœï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ´¾äººåŽ»æŽ¥åº”æ€¥æ•‘äººå‘˜ï¼ˆâ€œéº»çƒ¦æ‚¨åŽ»æ‹¨æ‰“æ€¥æ•‘ç”µè¯ï¼Œå¹¶æŠŠç»“æžœå‘Šè¯‰æˆ‘â€ï¼‰ å¯»æ±‚å¦ä¸€æ•‘æŠ¤å‘˜å¸®åŠ©ï¼ˆâ€œæœ‰è°å­¦è¿‡æ€¥æ•‘ï¼Œè¿‡æ¥å¸®ä¸‹å¿™â€ï¼‰ å–å›žAEDï¼ˆâ€œé™„è¿‘æœ‰AEDçš„è¯éº»çƒ¦æ‹¿è¿‡æ¥â€ï¼‰ å¿ƒè‚ºå¤è‹åŽï¼Œå‘çŽ°å‘¼å¸å¿ƒè·³æ¢å¤ï¼Œå°†æ‚£è€…è¡£ç‰©æ¢å¤ï¼Œè½¬æˆä¾§å§ä½ï¼Œé¿å…çª’æ¯ å¿ƒè„ä¸é€‚å¦‚æžœæœ‰äººå¿ƒè„ä¸é€‚ï¼Œè¯¢é—®æƒ…å†µåŽï¼ŒååŠ©å…¶æœç”¨éšèº«è¯ç‰© é€Ÿæ•ˆæ•‘å¿ƒä¸¸ï¼šæ­£å¸¸æƒ…å†µ3-5ç²’ï¼Œç´§æ€¥æƒ…å†µ10-15ç²’ï¼ŒèˆŒä¸‹å«æœ ç¡é…¸ç”˜æ²¹ï¼šä¸€æ¬¡ä¸€ç‰‡ï¼Œå¦‚æžœæ— æ•ˆçš„è¯äº”åˆ†é’ŸåŽå†æœä¸€ç‰‡ï¼Œæœ€å¤šä½¿ç”¨ä¸‰ç‰‡ ä¸­é£Ž æ²¡æœ‰ç‰¹å®šè¯ç‰© è®©æ‚£è€…ä¸¾æ‰‹ï¼Œå­¦è¯ï¼Œè¯´â€œä¸€â€/å¾®ç¬‘ï¼Œè§‚å¯Ÿè‚Œè‚‰æ˜¯å¦æ­£å¸¸ï¼Œå¦‚æžœä¸æ­£å¸¸ï¼Œå¤§æ¦‚çŽ‡æ˜¯é¢…è„‘å‡ºçŽ°é—®é¢˜ï¼Œå¯ä½œåˆæ­¥è¯Šæ–­æä¾›ç»™åŽç»­æ•‘æ²»çš„åŒ»ç”Ÿ å‘¼å¸é“å¼‚ç‰©é˜»å¡ž document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>in Chinese</category>
        <category>skills</category>
      </categories>
      <tags>
        <tag>CPR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Two NN-based VIO Methods]]></title>
    <url>%2F2019%2F03%2F13%2FTwo-NN-based-VIO-Methods%2F</url>
    <content type="text"><![CDATA[[In Chinese]Notes about two NN-based VIO methods:1) VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem2) Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problemhttps://arxiv.org/pdf/1701.08376.pdf ä¸»è¦è´¡çŒ® ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„VIOç³»ç»Ÿï¼ŒæŠŠVIOçœ‹åšæ˜¯Seq2Seqçš„å›žå½’é—®é¢˜ åˆ©ç”¨RNNï¼ˆLSTMï¼‰æå–åºåˆ—ä¸­çš„æ—¶åºå˜åŒ–å…³ç³»ï¼Œå…ˆé€šè¿‡CNNæå–å›¾åƒçš„ç‰¹å¾ï¼Œç„¶åŽä¸ŽLSTMæå–çš„IMUç‰¹å¾æ‹¼æŽ¥èµ·æ¥ï¼Œå†é€šè¿‡å¦ä¸€ä¸ªLSTMè¾“å‡ºæœ€ç»ˆçš„poseï¼Œé€šè¿‡ä¸¤ä¸ªLSTMè§£å†³äº†IMUçš„é¢‘çŽ‡é«˜äºŽç›¸æœºé¢‘çŽ‡çš„é—®é¢˜ï¼Œå³åˆ©ç”¨LSTMå®žçŽ°äº†ç±»ä¼¼pre-integrationçš„ä½œç”¨ å…¶å®žå°±æ˜¯åˆ©ç”¨RNNé¿å…äº†å¤æ‚çš„åŠ¨åŠ›å­¦å»ºæ¨¡ æå‡ºäº†ä¸€ä¸ªæ–°çš„å¯å¾®çš„ä½å§¿concatå±‚ï¼Œå®ƒå…è®¸ç½‘ç»œçš„é¢„æµ‹ç¬¦åˆSE(3)æµå½¢çš„ç»“æž„ï¼ˆè¾“å‡ºæ›´æ˜“å¾—åˆ°çš„se(3)ï¼Œç„¶åŽé€šè¿‡expå±‚å¾—åˆ°SE(3)ï¼‰ ç½‘ç»œç»“æž„ç½‘ç»œè¾“å…¥å•ç›®ç›¸æœºçš„å‰åŽä¸¤å¸§RGBå›¾åƒï¼Œå’Œ6Dçš„IMUæ•°æ®ï¼ˆ3DåŠ é€Ÿåº¦å’Œ3Dè§’é€Ÿåº¦ï¼‰ï¼Œç½‘ç»œè¾“å‡ºä¸º7Dçš„çŸ¢é‡ï¼ˆ3Dçš„ä½ç§»çŸ¢é‡å’Œè¡¨ç¤ºæ—‹è½¬çš„å››å…ƒæ•°ï¼‰ å®žçŽ°ç»†èŠ‚ ä½¿ç”¨ä¸¤å±‚çš„LSTMï¼Œå«æœ‰1000ä¸ªcell åœ¨å‰å‘æ—¶CNNéƒ¨åˆ†æ¶ˆè€—äº†160ms($\approx10Hz$)å·¦å³çš„æ—¶é—´ï¼Œè€ŒLSTMéƒ¨åˆ†å¯ä»¥è¶…è¿‡200Hzï¼Œéƒ½æ˜¯åœ¨Tesla k80ä¸Šæµ‹å¾—çš„ç»“æžœ åˆ†åˆ«ä½¿ç”¨se(3)å’ŒSE(3)è®¡ç®—äº†lossï¼Œåœ¨è®­ç»ƒåˆæœŸSE(3)çš„lossæ¯”é‡å°ï¼ŒåŽæœŸå¢žå¤§ å­˜åœ¨çš„é—®é¢˜ è®ºæ–‡ä¸­å¯¹äºŽç²¾åº¦çš„è¯´æ˜Žä¸å¤ªå¤šï¼Œå¯¹æ¯”çš„ä¹Ÿä¸æ˜¯ç›®å‰ç²¾åº¦æœ€å¥½çš„ç»“æžœ ä»ç„¶éœ€è¦ç›¸æœºå’ŒIMUä¹‹é—´çš„å¤–éƒ¨æ ‡å®šå‚æ•°ï¼Œè€Œä¸”æ˜¯æœ‰ç›‘ç£çš„è®­ç»ƒï¼ˆè¿™ä¸ªè¯„ä»·æ¥è‡ªä¸‹é¢çš„Vision-Aided ATEï¼‰# æ ‡å®šå‚æ•°æ˜¯æ€Žä¹ˆç”¨åˆ°çš„æˆ‘è¿˜æ²¡æœ‰å‘çŽ°ï¼Œå¯ä»¥åœ¨ä¹‹åŽå°è¯•çš„æ—¶å€™å…³æ³¨ä¸€ä¸‹â€¦ Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correctionhttps://arxiv.org/pdf/1803.05850.pdf ä¸»è¦è´¡çŒ® æå‡ºäº†ä¸€ç§åœ¨çº¿çº é”™çš„æ–¹æ³•ï¼Œå¯ä»¥æé«˜æ¨¡åž‹çš„é²æ£’æ€§ ä¸éœ€è¦ä½¿ç”¨IMUçš„å†…å‚å’Œä¸Žç›¸æœºä¹‹é—´æ ‡å®šçš„å¤–å‚ æ¾æ•£çš„ç›¸æœºå’ŒIMUçš„æ—¶é—´åŒæ­¥ ä½¿ç”¨æ— ç›‘ç£è®­ç»ƒæ¡†æž¶å®žçŽ°VIOç³»ç»Ÿ ç½‘ç»œç»“æž„ Overview Detailsé€šè¿‡CNNå…ˆå¯¹è¾“å…¥çš„IMUæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå…ˆè¾“å‡ºå¯¹poseçš„ç²—ç•¥ä¼°è®¡ï¼Œç„¶åŽä¸Žsource RGB-D imageå’Œsource coordinatesé€šè¿‡STNè¿›è¡Œå›¾åƒé‡å»ºï¼Œå°†é‡å»ºè¯¯å·®æ±‚Jacobianï¼Œç„¶åŽå†å°†Jacobianè¾“å…¥åˆ°ä¸‹ä¸€ä¸ªlevelï¼Œå®žçŽ°å¢žé‡ä¼°è®¡ï¼Œé€ä¸ªlevelä¼ é€’åŽï¼Œä½¿ç”¨æ¯ä¸ªlevelçš„ä½å§¿ä¼°è®¡ä¸­é‡å»ºè¯¯å·®æœ€å°çš„ä½å§¿ä½œä¸ºæœ€åŽçš„è¾“å‡ºï¼Œåœ¨inferenceæ—¶ä¹Ÿè¾“å‡ºå¤šä¸ªå°ºå¯¸ï¼Œç„¶åŽå–å…¶æœ€ä¼˜ï¼Œå³æ‰€è°“çš„online correction. å®žéªŒç»“æžœåŸºæœ¬é™¤äº†æ¯”ORB-SLAM2è¦å·®ï¼Œå’Œå…¶ä»–æ–¹æ³•æ¯”è¿˜æ˜¯æœ‰ä¼˜åŠ¿çš„ï¼Œä¸è¿‡æ²¡æœ‰å’ŒVINSç³»ç»Ÿçš„å¯¹æ¯”ç»“æžœ å­˜åœ¨çš„é—®é¢˜ ä½¿ç”¨RGB-Dä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨SfMçš„æ–¹æ³•ï¼Œé€šè¿‡ç½‘ç»œé¢„æµ‹æ·±åº¦å›¾ï¼Œä»Žè€Œå®žçŽ°åŸºäºŽçº¯å•ç›®RGBç›¸æœºçš„VIO å‰å‘æ—¶è¾“å‡ºå¤šä¸ªå°ºå¯¸ï¼Œç›¸å½“äºŽå¢žåŠ äº†å‡ å€çš„è®¡ç®—é‡ï¼Œè®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºæ—¶é—´çš„ç»“æžœï¼Œæ„Ÿè§‰ä¸èƒ½åšçš„å¾ˆå¿« IMUæœ¬èº«å°±èƒ½æŽ¨å‡ºä¸€ä¸ªç²—ç•¥çš„ä½å§¿ï¼Œä½¿ç”¨CNNå¤„ç†IMUæ•°æ®æ˜¯ä¸æ˜¯åˆé€‚ï¼Ÿå¯ä»¥ä¹‹åŽå°è¯•ä¸€ä¸‹ï¼ˆ#TODOï¼‰ å¯¹æ¯”åŠæ€è€ƒ ä¸¤ç¯‡æ–‡ç« éƒ½æ²¡æœ‰ä½¿ç”¨å¤æ‚çš„åŠ¨åŠ›å­¦å»ºæ¨¡ï¼Œè€Œæ˜¯åˆ†åˆ«ä½¿ç”¨LSTMå’ŒCNNâ€œç®€åŒ–â€äº†IMUæ•°æ®çš„å¤„ç†è¿‡ç¨‹ï¼Œç›®å‰çœ‹æ•ˆæžœä¹Ÿè¿˜å¯ä»¥ï¼Œä¸è¿‡IMUæœ¬èº«å¯ä»¥æŽ¨æ–­å‡ºä¸¤ä¸ªè‡ªç”±åº¦çš„ä½å§¿ï¼ˆpitch&amp;rollï¼‰ï¼Œåˆ©ç”¨æ•°å­¦å»ºæ¨¡å’Œæ·±åº¦ç½‘ç»œç»“åˆæ˜¯ä¸æ˜¯ä¼šæ›´ç¨³å¦¥ï¼Ÿ ä½¿ç”¨æ— ç›‘ç£ä¼°è®¡æ·±åº¦å’Œä½å§¿ï¼Œç„¶åŽå°†æ·±åº¦å›¾ä¸Žè¾“å…¥çš„RGBå›¾æ”¾åœ¨ä¸€èµ·å³å¯ä½¿ç”¨åŽŸæœ¬åŸºäºŽRGB-Då›¾åƒçš„æ–¹æ³•ï¼Œä¹‹åŽå¯ä»¥å¤šçœ‹çœ‹è¿™æ–¹é¢çš„ä¸œè¥¿ å¯¹äºŽå¤šæœºmappingï¼Œå¦‚æžœåŸºäºŽæ·±åº¦å›¾å»ºå›¾ï¼Œèƒ½ä¸èƒ½åˆ©ç”¨æ·±åº¦é¢„æµ‹ç½‘ç»œçš„bottleneckç‰¹å¾ä½œä¸ºä¼ è¾“æ•°æ®ï¼Œç„¶åŽåœ¨æŽ¥æ”¶åŽdecoderï¼Œä¹Ÿè®¸è¿˜èƒ½å¯¹ç‰¹å¾è¿›ä¸€æ­¥åŽ‹ç¼©ï¼Œæˆ–è®¸å¯è¡Œï¼Ÿ ç›´è§‰ä¸Šçœ‹ï¼ŒLSTMåº”è¯¥æ›´é€‚åˆè¿™ç§åŸºäºŽè§†é¢‘å¸§çš„ä»»åŠ¡ï¼Œå¯ä»¥ä¸»è¦å°è¯•ä¸€ä¸‹ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>paper-reading</category>
        <category>in Chinese</category>
      </categories>
      <tags>
        <tag>VIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>blog-building</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
